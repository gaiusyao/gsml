{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 降低损失"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 迭代方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;下图显示了机器学习算法用于训练模型的迭代试错过程：\n",
    "![图 1. 用于训练模型的迭代方法](http://oxv2o8wp9.bkt.clouddn.com/gsml_04.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;迭代策略在机器学习中的应用非常普遍，这主要是因为它们可以很好地扩展到大型数据集。\n",
    "\n",
    "&emsp;&emsp;“模型”部分将一个或多个特征作为输入，然后返回一个预测 (y') 作为输出。为了进行简化，不妨考虑一种采用一个特征并返回一个预测的模型：\n",
    "\n",
    "> $y' = b + w_1x_1$\n",
    "\n",
    "&emsp;&emsp;对于线性回归问题，事实证明初始值并不重要。我们可以随机选择值，不过我们还是选择采用以下这些无关紧要的值：\n",
    "\n",
    "- $b = 0$\n",
    "- $w_1 = 0$\n",
    "\n",
    "&emsp;&emsp;假设第一个特征值是 10。将该特征值代入预测函数会得到以下结果：\n",
    "\n",
    "```\n",
    "  y' = 0 + 0(10)\n",
    "  y' = 0\n",
    "```\n",
    "\n",
    "&emsp;&emsp;图中的“计算损失”部分是模型将要使用的损失函数。假设我们使用平方损失函数。损失函数将采用两个输入值：\n",
    "\n",
    "- $y'$：模型对特征 x 的预测\n",
    "- $y$：特征 x 对应的正确标签。\n",
    "\n",
    "&emsp;&emsp;最后，我们来看图的“计算参数更新”部分。机器学习系统就是在此部分检查损失函数的值，并为 $b$ 和 $w_1$ 生成新值。现在，假设这个神秘的绿色框会产生新值，然后机器学习系统将根据所有标签重新评估所有特征，为损失函数生成一个新值，而该值又产生新的参数值。这种学习过程会持续迭代，直到该算法发现损失可能最低的模型参数。通常，你可以不断迭代，直到总体损失不再变化或至少变化极其缓慢为止。这时候，我们可以说该模型已**收敛**。\n",
    "\n",
    "> **要点**：在训练机器学习模型时，首先对权重和偏差进行初始猜测，然后反复调整这些猜测，直到获得损失可能最低的权重和偏差为止。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;假设我们有时间和计算资源来计算 $w_1$ 的所有可能值的损失。对于我们一直在研究的回归问题，所产生的损失与 $w_1$ 的图形始终是凸形。换言之，图形始终是碗状图，如下所示：\n",
    "![图 2. 回归问题产生的损失与权重图为凸形](http://oxv2o8wp9.bkt.clouddn.com/gsml_05.png)\n",
    "\n",
    "&emsp;&emsp;凸形问题只有一个最低点；即只存在一个斜率正好为 0 的位置。这个最小值就是损失函数收敛之处。\n",
    "\n",
    "&emsp;&emsp;通过计算整个数据集中 $w_1$ 每个可能值的损失函数来找到收敛点这种方法效率太低。我们来研究一种更好的机制，这种机制在机器学习领域非常热门，称为**梯度下降法**。\n",
    "\n",
    "&emsp;&emsp;梯度下降法的第一个阶段是为 $w_1$ 选择一个起始值（起点）。起点并不重要；因此很多算法就直接将 $w_1$ 设为 0 或随机选择一个值。下图显示的是我们选择了一个稍大于 0 的起点：\n",
    "![图 3. 梯度下降法的起点](http://oxv2o8wp9.bkt.clouddn.com/gsml_06.png)\n",
    "\n",
    "&emsp;&emsp;然后，梯度下降法算法会计算损失曲线在起点处的梯度。简而言之，梯度是偏导数的矢量；它可以让你了解哪个方向距离目标“更近”或“更远”。请注意，损失相对于单个权重的梯度（如上图所示）就等于导数。\n",
    "\n",
    "&emsp;&emsp;请注意，梯度是一个矢量，因此具有以下两个特征：\n",
    "\n",
    "- 方向\n",
    "- 大小\n",
    "\n",
    "&emsp;&emsp;梯度始终指向损失函数中增长最为迅猛的方向。梯度下降法算法会沿着负梯度的方向走一步，以便尽快降低损失。\n",
    "![图 4. 梯度下降法依赖于负梯度](http://oxv2o8wp9.bkt.clouddn.com/gsml_07.png)\n",
    "\n",
    "&emsp;&emsp;为了确定损失函数曲线上的下一个点，梯度下降法算法会将梯度大小的一部分与起点相加，如下图所示：\n",
    "![图 5. 一个梯度步长将我们移动到损失曲线上的下一个点](http://oxv2o8wp9.bkt.clouddn.com/gsml_08.png)\n",
    "\n",
    "&emsp;&emsp;然后，梯度下降法会重复此过程，逐渐接近最低点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学习速率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;正如之前所述，梯度矢量具有方向和大小。梯度下降法算法用梯度乘以一个称为学习速率（有时也称为步长）的标量，以确定下一个点的位置。例如，如果梯度大小为 2.5，学习速率为 0.01，则梯度下降法算法会选择距离前一个点 0.025 的位置作为下一个点。\n",
    "\n",
    "&emsp;&emsp;**超参数**是编程人员在机器学习算法中用于调整的旋钮。大多数机器学习编程人员会花费相当多的时间来调整学习速率。如果选择的学习速率过小，就会花费太长的学习时间：\n",
    "![图 6. 学习速率过小](http://oxv2o8wp9.bkt.clouddn.com/gsml_09.png)\n",
    "\n",
    "&emsp;&emsp;相反，如果您指定的学习速率过大，下一个点将永远在 U 形曲线的底部随意弹跳，就好像量子力学实验出现了严重错误一样：\n",
    "![图 7. 学习速率过大](http://oxv2o8wp9.bkt.clouddn.com/gsml_10.png)\n",
    "\n",
    "&emsp;&emsp;每个回归问题都存在一个金发姑娘学习速率。“金发姑娘”值与损失函数的平坦程度相关。如果知道损失函数的梯度较小，则可以放心地试着采用更大的学习速率，以补偿较小的梯度并获得更大的步长。\n",
    "![图 8. 学习速率合适](http://oxv2o8wp9.bkt.clouddn.com/gsml_11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化学习速率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[优化学习速率练习](https://developers.google.cn/machine-learning/crash-course/fitter/graph)\n",
    "\n",
    "> **注意：**在实践中，成功的模型训练并不意味着要找到“完美”（或接近完美）的学习速率。我们的目标是找到一个足够高的学习速率，该速率要能够使梯度下降过程高效收敛，但又不会高到使该过程永远无法收敛。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 随机梯度下降法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在梯度下降法中，**批量**指的是用于在单次迭代中计算梯度的样本总数。到目前为止，我们一直假定批量是指整个数据集。就 Google 的规模而言，数据集通常包含数十亿甚至数千亿个样本。此外，Google 数据集通常包含海量特征。因此，一个批量可能相当巨大。如果是超大批量，则单次迭代就可能要花费很长时间进行计算。\n",
    "\n",
    "&emsp;&emsp;包含随机抽样样本的大型数据集可能包含冗余数据。实际上，批量大小越大，出现冗余的可能性就越高。一些冗余可能有助于消除杂乱的梯度，但超大批量所具备的预测价值往往并不比大型批量高。\n",
    "\n",
    "&emsp;&emsp;如果我们可以通过更少的计算量得出正确的平均梯度，会怎么样？通过从我们的数据集中随机选择样本，我们可以通过小得多的数据集估算（尽管过程非常杂乱）出较大的平均值。 **随机梯度下降法 (SGD) **将这种想法运用到极致，它每次迭代只使用一个样本（批量大小为 1）。如果进行足够的迭代，SGD 也可以发挥作用，但过程会非常杂乱。“随机”这一术语表示构成各个批量的一个样本都是随机选择的。\n",
    "\n",
    "&emsp;&emsp;**小批量随机梯度下降法（小批量 SGD）**是介于全批量迭代与 SGD 之间的折衷方案。小批量通常包含 10-1000 个随机选择的样本。小批量 SGD 可以减少 SGD 中的杂乱样本数量，但仍然比全批量更高效。\n",
    "\n",
    "&emsp;&emsp;为了简化说明，我们只针对单个特征重点介绍了梯度下降法。请放心，梯度下降法也适用于包含多个特征的特征集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[降低损失 (Reducing Loss)：Playground 练习](https://developers.google.cn/machine-learning/crash-course/reducing-loss/playground-exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检查理解情况"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 批量大小"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;基于大型数据集执行梯度下降法时，以下哪个批量大小可能比较高效？\n",
    "- [x] 小批量或甚至包含一个样本的批量 (SGD)。\n",
    "- [ ] 全批量。\n",
    "\n",
    "> 令人惊讶的是，在小批量或甚至包含一个样本的批量上执行梯度下降法通常比全批量更高效。毕竟，计算一个样本的梯度要比计算数百万个样本的梯度成本低的多。为确保获得良好的代表性样本，该算法在每次迭代时都会抽取另一个随机小批量数据（或包含一个样本的批量数据）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
